{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Cell 1: Imports & Data (same as your existing cells 1-6)\n",
    "# ============================================================\n",
    "import sys\n",
    "sys.path.append(\".\")\n",
    "\n",
    "import os, json, time\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from src.data.cmems_dataset import load_cmems_uv\n",
    "from src.models.unet_convlstm_unc import UNetConvLSTMUncertainty, gaussian_nll\n",
    "from src.data.cmems_dataset_ar import SlidingWindowMultiStep\n",
    "\n",
    "# --- Data loading (same as before) ---\n",
    "data_path = \"/home/svillhauer/Desktop/Thesis/Currents/deep_spatiotemporal_currents_uncertainty/cmems_mod_glo_phy_anfc_merged-uv_PT1H-i_1771446846329.nc\"\n",
    "\n",
    "uv, time_arr, lat, lon = load_cmems_uv(\n",
    "    data_path, u_var=\"utotal\", v_var=\"vtotal\",\n",
    "    depth_index=0, regrid_hw=(64, 64)\n",
    ")\n",
    "print(uv.shape)\n",
    "\n",
    "# --- Z-score normalisation ---\n",
    "class ZScoreStats:\n",
    "    def __init__(self, u_mean, u_std, v_mean, v_std):\n",
    "        self.u_mean = u_mean\n",
    "        self.u_std = u_std\n",
    "        self.v_mean = v_mean\n",
    "        self.v_std = v_std\n",
    "\n",
    "def compute_zscore(uv):\n",
    "    u, v = uv[:,0], uv[:,1]\n",
    "    return ZScoreStats(float(np.mean(u)), float(np.std(u)+1e-8),\n",
    "                       float(np.mean(v)), float(np.std(v)+1e-8))\n",
    "\n",
    "def apply_zscore(uv, stats):\n",
    "    uv_n = uv.copy()\n",
    "    uv_n[:,0] = (uv_n[:,0] - stats.u_mean) / stats.u_std\n",
    "    uv_n[:,1] = (uv_n[:,1] - stats.v_mean) / stats.v_std\n",
    "    return uv_n\n",
    "\n",
    "seq_len = 12\n",
    "max_horizon = 12\n",
    "split_t = int(len(uv) * 0.7)\n",
    "uv_train, uv_val = uv[:split_t], uv[split_t:]\n",
    "\n",
    "stats = compute_zscore(uv_train)\n",
    "uv_train_n = apply_zscore(uv_train, stats)\n",
    "uv_val_n   = apply_zscore(uv_val, stats)\n",
    "\n",
    "train_ds = SlidingWindowMultiStep(uv_train_n, seq_len=seq_len, max_horizon=max_horizon)\n",
    "val_ds   = SlidingWindowMultiStep(uv_val_n,   seq_len=seq_len, max_horizon=max_horizon)\n",
    "\n",
    "print(f\"Train: {len(train_ds)} samples,  Val: {len(val_ds)} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Cell 2: Training functions\n",
    "# ============================================================\n",
    "\n",
    "def ar_forward_loss(model, X, Y_seq, ar_steps):\n",
    "    \"\"\"Autoregressive forward pass with accumulated NLL loss (differentiable).\"\"\"\n",
    "    current_input = X\n",
    "    total_nll = torch.tensor(0.0, device=X.device)\n",
    "    total_mse = torch.tensor(0.0, device=X.device)\n",
    "\n",
    "    for t in range(ar_steps):\n",
    "        mu, logvar = model(current_input)\n",
    "        target = Y_seq[:, t]\n",
    "        total_nll = total_nll + gaussian_nll(mu, logvar, target)\n",
    "        total_mse = total_mse + torch.mean((mu - target) ** 2)\n",
    "\n",
    "        next_frame = mu.unsqueeze(1)\n",
    "        current_input = torch.cat([current_input[:, 1:], next_frame], dim=1)\n",
    "\n",
    "    return total_nll / ar_steps, torch.sqrt(total_mse / ar_steps)\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def validate(model, loader, ar_steps, device):\n",
    "    model.eval()\n",
    "    total_nll, total_rmse, n = 0.0, 0.0, 0\n",
    "    for X, Y_seq in loader:\n",
    "        X, Y_seq = X.to(device), Y_seq.to(device)\n",
    "        nll, r = ar_forward_loss(model, X, Y_seq, ar_steps)\n",
    "        bs = X.shape[0]\n",
    "        total_nll += nll.item() * bs\n",
    "        total_rmse += r.item() * bs\n",
    "        n += bs\n",
    "    return total_nll / max(n,1), total_rmse / max(n,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Cell 3: Curriculum training loop\n",
    "# ============================================================\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device:\", device)\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=4, shuffle=True)\n",
    "val_loader   = DataLoader(val_ds,   batch_size=4, shuffle=False)\n",
    "\n",
    "model = UNetConvLSTMUncertainty(base_ch=32, lstm_ch=256).to(device)\n",
    "opt   = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "# --- Config ---\n",
    "run_dir = \"runs/curriculum_unc\"\n",
    "os.makedirs(run_dir, exist_ok=True)\n",
    "history_path = os.path.join(run_dir, \"history.jsonl\")\n",
    "if os.path.exists(history_path):\n",
    "    os.remove(history_path)\n",
    "\n",
    "curriculum       = [1, 2, 4, 8, 12]   # AR steps per stage\n",
    "epochs_per_stage = 100\n",
    "val_every        = 5\n",
    "grad_clip        = 1.0\n",
    "lr_decay         = 0.5                 # multiply LR by this at each new stage\n",
    "\n",
    "best_val_rmse = float(\"inf\")\n",
    "global_epoch  = 0\n",
    "\n",
    "for stage_idx, ar_steps in enumerate(curriculum):\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"STAGE {stage_idx}: ar_steps = {ar_steps}\")\n",
    "    print(f\"{'='*60}\")\n",
    "\n",
    "    # Decay LR after first stage\n",
    "    if stage_idx > 0:\n",
    "        for pg in opt.param_groups:\n",
    "            pg[\"lr\"] *= lr_decay\n",
    "        print(f\"  LR -> {opt.param_groups[0]['lr']:.2e}\")\n",
    "\n",
    "    for local_epoch in range(epochs_per_stage):\n",
    "        t0 = time.time()\n",
    "        model.train()\n",
    "        train_nll_sum, n_train = 0.0, 0\n",
    "\n",
    "        for X, Y_seq in train_loader:\n",
    "            X, Y_seq = X.to(device), Y_seq.to(device)\n",
    "\n",
    "            nll, _ = ar_forward_loss(model, X, Y_seq, ar_steps)\n",
    "\n",
    "            opt.zero_grad(set_to_none=True)\n",
    "            nll.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n",
    "            opt.step()\n",
    "\n",
    "            bs = X.shape[0]\n",
    "            train_nll_sum += nll.item() * bs\n",
    "            n_train += bs\n",
    "\n",
    "        train_nll = train_nll_sum / max(n_train, 1)\n",
    "        elapsed = time.time() - t0\n",
    "\n",
    "        # --- Validate ---\n",
    "        do_val = (local_epoch % val_every == 0) or (local_epoch == epochs_per_stage - 1)\n",
    "        if do_val:\n",
    "            val_nll, val_rmse = validate(model, val_loader, ar_steps, device)\n",
    "        else:\n",
    "            val_nll, val_rmse = float(\"nan\"), float(\"nan\")\n",
    "\n",
    "        # --- Print ---\n",
    "        if do_val:\n",
    "            print(f\"  [{global_epoch:04d}] ar={ar_steps} train_nll={train_nll:.6f} \"\n",
    "                  f\"val_nll={val_nll:.6f} val_rmse={val_rmse:.6f} ({elapsed:.1f}s)\", flush=True)\n",
    "        else:\n",
    "            print(f\"  [{global_epoch:04d}] ar={ar_steps} train_nll={train_nll:.6f} ({elapsed:.1f}s)\", flush=True)\n",
    "\n",
    "        # --- Save history (appended every epoch â€” safe if kernel dies) ---\n",
    "        row = {\n",
    "            \"global_epoch\": global_epoch, \"stage\": stage_idx, \"ar_steps\": ar_steps,\n",
    "            \"train_nll\": float(train_nll),\n",
    "            \"val_nll\": float(val_nll) if do_val else None,\n",
    "            \"val_rmse\": float(val_rmse) if do_val else None,\n",
    "            \"lr\": float(opt.param_groups[0][\"lr\"]),\n",
    "        }\n",
    "        with open(history_path, \"a\") as f:\n",
    "            f.write(json.dumps(row) + \"\\n\")\n",
    "\n",
    "        # --- Checkpoints ---\n",
    "        ckpt = {\n",
    "            \"global_epoch\": global_epoch, \"stage\": stage_idx, \"ar_steps\": ar_steps,\n",
    "            \"model_state_dict\": model.state_dict(),\n",
    "            \"optimizer_state_dict\": opt.state_dict(),\n",
    "            \"best_val_rmse\": best_val_rmse if best_val_rmse < float(\"inf\") else None,\n",
    "            \"stats\": stats.__dict__, \"seq_len\": seq_len, \"max_horizon\": max_horizon,\n",
    "            \"curriculum\": curriculum,\n",
    "        }\n",
    "        torch.save(ckpt, os.path.join(run_dir, \"last.pt\"))\n",
    "\n",
    "        if do_val and val_rmse < best_val_rmse:\n",
    "            best_val_rmse = val_rmse\n",
    "            torch.save(ckpt, os.path.join(run_dir, \"best.pt\"))\n",
    "            print(f\"    -> New best val_rmse={val_rmse:.6f}\")\n",
    "\n",
    "        global_epoch += 1\n",
    "\n",
    "    # End-of-stage checkpoint\n",
    "    torch.save(ckpt, os.path.join(run_dir, f\"stage_{stage_idx}_ar{ar_steps}.pt\"))\n",
    "    print(f\"  Saved stage checkpoint: stage_{stage_idx}_ar{ar_steps}.pt\")\n",
    "\n",
    "print(f\"\\nDone! Best val_rmse = {best_val_rmse:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
