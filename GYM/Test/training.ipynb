{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from stable_baselines3 import SAC\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "from stable_baselines3.common.results_plotter import load_results, ts2xy\n",
    "from asv_glider_bearing_dist_env import AsvGliderBearingEnv\n",
    "\n",
    "# --- 1. Define the Rollout Callback ---\n",
    "class TrajectoryPlotCallback(BaseCallback):\n",
    "    def __init__(self, eval_env, render_freq=10000, log_dir=\"./rollouts/\"):\n",
    "        super().__init__()\n",
    "        self.eval_env = eval_env\n",
    "        self.render_freq = render_freq\n",
    "        self.log_dir = log_dir\n",
    "        os.makedirs(self.log_dir, exist_ok=True)\n",
    "\n",
    "    def _on_step(self) -> bool:\n",
    "        # Check if it's time to run a rollout\n",
    "        if self.n_calls % self.render_freq == 0:\n",
    "            obs, _ = self.eval_env.reset()\n",
    "            asv_history = []\n",
    "            glider_history = []\n",
    "            done = False\n",
    "            \n",
    "            # Run one full test episode deterministically\n",
    "            while not done:\n",
    "                action, _ = self.model.predict(obs, deterministic=True)\n",
    "                obs, reward, terminated, truncated, _ = self.eval_env.step(action)\n",
    "                \n",
    "                # Store un-normalized positions for plotting\n",
    "                asv_history.append(self.eval_env.unwrapped.asv_pos.copy())\n",
    "                glider_history.append(self.eval_env.unwrapped.glider_pos.copy())\n",
    "                done = terminated or truncated\n",
    "\n",
    "            # Convert to arrays for plotting\n",
    "            asv_history = np.array(asv_history)\n",
    "            glider_history = np.array(glider_history)\n",
    "\n",
    "            # Generate the trajectory plot\n",
    "            plt.figure(figsize=(6, 6))\n",
    "            plt.plot(glider_history[:, 0], glider_history[:, 1], 'g--', label=\"Glider Path\", alpha=0.6)\n",
    "            plt.plot(asv_history[:, 0], asv_history[:, 1], 'b-', label=\"ASV Path\", linewidth=2)\n",
    "            plt.scatter(asv_history[0, 0], asv_history[0, 1], c='blue', label=\"ASV Start\")\n",
    "            plt.scatter(glider_history[0, 0], glider_history[0, 1], c='green', label=\"Glider Start\")\n",
    "            \n",
    "            plt.title(f\"Rollout at Step {self.n_calls}\")\n",
    "            plt.xlabel(\"X Position (m)\")\n",
    "            plt.ylabel(\"Y Position (m)\")\n",
    "            plt.legend()\n",
    "            plt.grid(True)\n",
    "            \n",
    "            # Save the plot\n",
    "            save_path = os.path.join(self.log_dir, f\"rollout_{self.n_calls}.png\")\n",
    "            plt.savefig(save_path)\n",
    "            plt.close()\n",
    "            print(f\">>> Saved trajectory rollout to {save_path}\")\n",
    "\n",
    "        return True\n",
    "\n",
    "# --- 2. Setup Directories and Environments ---\n",
    "log_dir = \"./sac_asv_logs/\"\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "\n",
    "# Main training environment\n",
    "env = AsvGliderBearingEnv()\n",
    "env = Monitor(env, log_dir)\n",
    "\n",
    "# Separate environment for the callback to use (prevents interference with training state)\n",
    "eval_env = AsvGliderBearingEnv()\n",
    "\n",
    "# --- 3. Initialize Model and Callback ---\n",
    "model = SAC(\n",
    "    \"MlpPolicy\", # Policy type\n",
    "    env,\n",
    "    verbose=1,\n",
    "    learning_rate=7e-5, \n",
    "    gamma=0.99,\n",
    "    buffer_size=1_000_000,\n",
    "    learning_starts=10_000,\n",
    "    batch_size=256,\n",
    "    tau=0.005,\n",
    "    train_freq=1,\n",
    "    gradient_steps=1,\n",
    "    device=\"cuda\",\n",
    ")\n",
    "\n",
    "# Initialize callback: plots every 10k steps\n",
    "plot_callback = TrajectoryPlotCallback(eval_env, render_freq=10000)\n",
    "\n",
    "from stable_baselines3.common.callbacks import CallbackList, EvalCallback\n",
    "\n",
    "# --- 4. Setup Evaluation & Real-time Printing ---\n",
    "# This environment is used to calculate the \"mean reward\" periodically\n",
    "eval_env = AsvGliderBearingEnv()\n",
    "eval_env = Monitor(eval_env, \"./logs/eval/\") # Optional: logs eval results separately\n",
    "\n",
    "# 1. EvalCallback: This handles the mean reward printing you asked for\n",
    "eval_callback = EvalCallback(\n",
    "    eval_env, \n",
    "    best_model_save_path=\"./logs/best_model\",\n",
    "    log_path=\"./logs/results\", \n",
    "    eval_freq=5000,         # How often to calculate mean reward (every 5k steps)\n",
    "    deterministic=True, \n",
    "    render=False\n",
    ")\n",
    "\n",
    "# 2. Combine with your trajectory plotter\n",
    "callbacks = CallbackList([eval_callback, plot_callback])\n",
    "\n",
    "# --- 5. Train with Error Handling ---\n",
    "try:\n",
    "    print(\"Starting training. Look for 'Eval num_timesteps' in the output for mean rewards.\")\n",
    "    model.learn(\n",
    "        total_timesteps=100_000, \n",
    "        callback=callbacks,\n",
    "        progress_bar=True  # Adds a nice loading bar in the notebook\n",
    "    )\n",
    "except KeyboardInterrupt:\n",
    "    print(\"\\nTraining interrupted by user.\")\n",
    "finally:\n",
    "    # Forces the monitor.csv to write all remaining data to disk\n",
    "    env.close()\n",
    "    eval_env.close()\n",
    "    model.save(\"sac_asv_bearing_dist_final\")\n",
    "    print(\"Environment closed and logs flushed. You can now run the plotting code.\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
